{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses pre-processing selesai, file telah disimpan sebagai 'pre_processing_data.csv'\n"
     ]
    }
   ],
   "source": [
    "#step 1 preprocessing data\n",
    "\n",
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# Membaca dataset dari file .tsv\n",
    "raw_data = pd.read_csv(r'E:\\Bebeb\\NEW\\AES\\data\\asap-aes\\training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "# Menyaring data untuk hanya mengambil essay_set dengan nilai 7\n",
    "filtered_dataset = raw_data[raw_data['essay_set'] == 7].copy()\n",
    "\n",
    "# Menyimpan dataset yang telah disaring ke dalam file CSV\n",
    "filtered_dataset.to_csv('aes_dataset_value_7.csv', index=False)\n",
    "\n",
    "# Menentukan kolom yang akan digabungkan untuk skor Struktur\n",
    "skor_struktur = ['rater1_trait1', 'rater1_trait2', 'rater1_trait3', 'rater2_trait1', 'rater2_trait2', 'rater2_trait3']\n",
    "\n",
    "# Menghitung skor struktur sebagai rata-rata dari 6 karakteristik yang ada\n",
    "filtered_dataset.loc[:, 'skor_struktur'] = filtered_dataset[skor_struktur].sum(axis=1) / len(skor_struktur)\n",
    "\n",
    "# Menentukan kolom yang akan digabungkan untuk skor Tata Bahasa\n",
    "skor_tata_bahasa = ['rater1_trait4', 'rater2_trait4']\n",
    "\n",
    "# Menghitung skor tata bahasa sebagai rata-rata dari 2 karakteristik yang ada\n",
    "filtered_dataset.loc[:, 'skor_tata_bahasa'] = filtered_dataset[skor_tata_bahasa].sum(axis=1) / len(skor_tata_bahasa)\n",
    "\n",
    "# Normalisasi skor Struktur dalam rentang 0-10\n",
    "min_value_struktur = filtered_dataset['skor_struktur'].min()\n",
    "max_value_struktur = filtered_dataset['skor_struktur'].max()\n",
    "filtered_dataset['skor_struktur_normalized'] = 10 * (filtered_dataset['skor_struktur'] - min_value_struktur) / (max_value_struktur - min_value_struktur)\n",
    "filtered_dataset['skor_struktur_normalized'] = filtered_dataset['skor_struktur_normalized'].round(1)\n",
    "\n",
    "# Normalisasi skor Tata Bahasa dalam rentang 0-10\n",
    "min_value_tata_bahasa = filtered_dataset['skor_tata_bahasa'].min()\n",
    "max_value_tata_bahasa = filtered_dataset['skor_tata_bahasa'].max()\n",
    "filtered_dataset['skor_tata_bahasa_normalized'] = 10 * (filtered_dataset['skor_tata_bahasa'] - min_value_tata_bahasa) / (max_value_tata_bahasa - min_value_tata_bahasa)\n",
    "filtered_dataset['skor_tata_bahasa_normalized'] = filtered_dataset['skor_tata_bahasa_normalized'].round(1)\n",
    "\n",
    "# Pilih kolom-kolom yang relevan untuk disimpan dalam dataset hasil pre-processing\n",
    "pre_processing_data = ['essay_id', 'essay_set', 'essay', 'skor_struktur_normalized', 'skor_tata_bahasa_normalized']\n",
    "\n",
    "# Membuat dataset akhir yang berisi data yang sudah diproses\n",
    "final_pre_processing_data = filtered_dataset[pre_processing_data]\n",
    "\n",
    "# Menyimpan data hasil pre-processing ke dalam file CSV\n",
    "final_pre_processing_data.to_csv('pre_processing_data.csv', index=False)\n",
    "\n",
    "print(\"Proses pre-processing selesai, file telah disimpan sebagai 'pre_processing_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 2060\n",
      "tensor([[0.4164, 0.4571, 0.7109],\n",
      "        [0.0342, 0.3336, 0.5446],\n",
      "        [0.1767, 0.2400, 0.8348]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Test if CUDA is available and the model is using the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Example tensor operation on GPU\n",
    "tensor = torch.rand(3, 3).to(device)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 6.5188, Training QWK Score: 0.0051\n",
      "Validation Loss: 5.4335, Validation QWK Score: -0.0266\n",
      "Epoch [2/50], Loss: 1.2425, Training QWK Score: 0.0018\n",
      "Validation Loss: 4.3754, Validation QWK Score: 0.1383\n",
      "Epoch [3/50], Loss: 0.9899, Training QWK Score: -0.0051\n",
      "Validation Loss: 4.2101, Validation QWK Score: 0.1767\n",
      "Epoch [4/50], Loss: 13.0072, Training QWK Score: 0.0187\n",
      "Validation Loss: 4.0831, Validation QWK Score: 0.1908\n",
      "Epoch [5/50], Loss: 0.3781, Training QWK Score: 0.0592\n",
      "Validation Loss: 4.0045, Validation QWK Score: 0.4635\n",
      "Epoch [6/50], Loss: 0.1065, Training QWK Score: 0.0684\n",
      "Validation Loss: 3.5354, Validation QWK Score: 0.7086\n",
      "Epoch [7/50], Loss: 0.0978, Training QWK Score: 0.2074\n",
      "Validation Loss: 2.1527, Validation QWK Score: 0.7338\n",
      "Epoch [8/50], Loss: 0.0045, Training QWK Score: 0.5910\n",
      "Validation Loss: 1.7707, Validation QWK Score: 0.7164\n",
      "Epoch [9/50], Loss: 2.2824, Training QWK Score: 0.6182\n",
      "Validation Loss: 2.2771, Validation QWK Score: 0.6988\n",
      "Epoch [10/50], Loss: 21.3255, Training QWK Score: 0.5594\n",
      "Validation Loss: 2.5685, Validation QWK Score: 0.7242\n",
      "Epoch [11/50], Loss: 0.4032, Training QWK Score: 0.4914\n",
      "Validation Loss: 3.1920, Validation QWK Score: 0.7239\n",
      "Epoch [12/50], Loss: 9.4549, Training QWK Score: 0.5618\n",
      "Validation Loss: 1.6439, Validation QWK Score: 0.7606\n",
      "Epoch [13/50], Loss: 2.1929, Training QWK Score: 0.6472\n",
      "Validation Loss: 1.6339, Validation QWK Score: 0.7627\n",
      "Epoch [14/50], Loss: 2.4490, Training QWK Score: 0.6531\n",
      "Validation Loss: 1.5481, Validation QWK Score: 0.7758\n",
      "Epoch [15/50], Loss: 0.0953, Training QWK Score: 0.6474\n",
      "Validation Loss: 1.5881, Validation QWK Score: 0.7764\n",
      "Epoch [16/50], Loss: 10.0748, Training QWK Score: 0.6916\n",
      "Validation Loss: 1.5491, Validation QWK Score: 0.7697\n",
      "Epoch [17/50], Loss: 0.1862, Training QWK Score: 0.5718\n",
      "Validation Loss: 1.4823, Validation QWK Score: 0.7900\n",
      "Epoch [18/50], Loss: 0.7296, Training QWK Score: 0.6386\n",
      "Validation Loss: 1.4851, Validation QWK Score: 0.7846\n",
      "Epoch [19/50], Loss: 5.4619, Training QWK Score: 0.6765\n",
      "Validation Loss: 1.5383, Validation QWK Score: 0.7810\n",
      "Epoch [20/50], Loss: 2.7003, Training QWK Score: 0.5633\n",
      "Validation Loss: 1.5656, Validation QWK Score: 0.7709\n",
      "Epoch [21/50], Loss: 0.7090, Training QWK Score: 0.6500\n",
      "Validation Loss: 1.4995, Validation QWK Score: 0.7889\n",
      "Epoch [22/50], Loss: 0.6396, Training QWK Score: 0.6948\n",
      "Validation Loss: 1.6649, Validation QWK Score: 0.7792\n",
      "Epoch [23/50], Loss: 1.0456, Training QWK Score: 0.6357\n",
      "Validation Loss: 1.7111, Validation QWK Score: 0.7807\n",
      "Epoch [24/50], Loss: 1.3677, Training QWK Score: 0.5985\n",
      "Validation Loss: 1.6773, Validation QWK Score: 0.7481\n",
      "Epoch [25/50], Loss: 0.0188, Training QWK Score: 0.5390\n",
      "Validation Loss: 1.9524, Validation QWK Score: 0.7684\n",
      "Epoch [26/50], Loss: 2.5093, Training QWK Score: 0.6596\n",
      "Validation Loss: 1.8750, Validation QWK Score: 0.7574\n",
      "Epoch [27/50], Loss: 2.5887, Training QWK Score: 0.6558\n",
      "Validation Loss: 1.6755, Validation QWK Score: 0.7732\n",
      "Epoch [28/50], Loss: 0.0041, Training QWK Score: 0.7011\n",
      "Validation Loss: 1.6226, Validation QWK Score: 0.7676\n",
      "Epoch [29/50], Loss: 0.5877, Training QWK Score: 0.7622\n",
      "Validation Loss: 1.7470, Validation QWK Score: 0.7683\n",
      "Epoch [30/50], Loss: 3.1585, Training QWK Score: 0.6799\n",
      "Validation Loss: 1.8367, Validation QWK Score: 0.7434\n",
      "Epoch [31/50], Loss: 0.5998, Training QWK Score: 0.6653\n",
      "Validation Loss: 1.4850, Validation QWK Score: 0.7653\n",
      "Epoch [32/50], Loss: 2.8202, Training QWK Score: 0.7433\n",
      "Validation Loss: 1.5139, Validation QWK Score: 0.7573\n",
      "Epoch [33/50], Loss: 0.3623, Training QWK Score: 0.7323\n",
      "Validation Loss: 1.6119, Validation QWK Score: 0.7659\n",
      "Epoch [34/50], Loss: 2.5577, Training QWK Score: 0.7441\n",
      "Validation Loss: 1.7569, Validation QWK Score: 0.7534\n",
      "Epoch [35/50], Loss: 0.0281, Training QWK Score: 0.7584\n",
      "Validation Loss: 2.0003, Validation QWK Score: 0.7594\n",
      "Epoch [36/50], Loss: 0.0403, Training QWK Score: 0.7157\n",
      "Validation Loss: 1.7130, Validation QWK Score: 0.7459\n",
      "Epoch [37/50], Loss: 3.0474, Training QWK Score: 0.6891\n",
      "Validation Loss: 1.8000, Validation QWK Score: 0.7563\n",
      "Epoch [38/50], Loss: 0.0820, Training QWK Score: 0.7291\n",
      "Validation Loss: 1.7701, Validation QWK Score: 0.7501\n",
      "Epoch [39/50], Loss: 0.5514, Training QWK Score: 0.7849\n",
      "Validation Loss: 2.3954, Validation QWK Score: 0.7481\n",
      "Epoch [40/50], Loss: 0.3876, Training QWK Score: 0.7631\n",
      "Validation Loss: 1.8617, Validation QWK Score: 0.7495\n",
      "Epoch [41/50], Loss: 1.1471, Training QWK Score: 0.7828\n",
      "Validation Loss: 1.8283, Validation QWK Score: 0.7728\n",
      "Epoch [42/50], Loss: 0.0485, Training QWK Score: 0.7394\n",
      "Validation Loss: 2.0753, Validation QWK Score: 0.7359\n",
      "Epoch [43/50], Loss: 0.0462, Training QWK Score: 0.7470\n",
      "Validation Loss: 2.1214, Validation QWK Score: 0.7672\n",
      "Epoch [44/50], Loss: 0.0363, Training QWK Score: 0.7276\n",
      "Validation Loss: 2.2624, Validation QWK Score: 0.7042\n",
      "Epoch [45/50], Loss: 0.2306, Training QWK Score: 0.7717\n",
      "Validation Loss: 1.5899, Validation QWK Score: 0.7726\n",
      "Epoch [46/50], Loss: 0.0235, Training QWK Score: 0.7873\n",
      "Validation Loss: 1.8408, Validation QWK Score: 0.7385\n",
      "Epoch [47/50], Loss: 2.1218, Training QWK Score: 0.7643\n",
      "Validation Loss: 1.6488, Validation QWK Score: 0.7375\n",
      "Epoch [48/50], Loss: 0.6128, Training QWK Score: 0.7650\n",
      "Validation Loss: 2.1152, Validation QWK Score: 0.6554\n",
      "Epoch [49/50], Loss: 0.0332, Training QWK Score: 0.7626\n",
      "Validation Loss: 1.7248, Validation QWK Score: 0.7451\n",
      "Epoch [50/50], Loss: 0.1604, Training QWK Score: 0.7293\n",
      "Validation Loss: 1.8022, Validation QWK Score: 0.7196\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Load preprocessed dataset\n",
    "data_preprocessing = pd.read_csv('pre_processing_data.csv')\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move BERT model to the device (CUDA or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Extract essay texts\n",
    "essay_texts = data_preprocessing['essay'].tolist()\n",
    "\n",
    "# Tokenize essays with padding and truncation\n",
    "inputs = tokenizer(essay_texts, return_tensors='pt', padding=True, max_length=512, truncation=True)\n",
    "\n",
    "# Extract token IDs and attention mask\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Move tokenized inputs to the device\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Define the Structure Scoring Model (LSTM + Dropout + Dense)\n",
    "class StructureScoreModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768):  # Input size (BERT hidden size)\n",
    "        super(StructureScoreModel, self).__init__()\n",
    "        # LSTM Layer 1 with 400 hidden units (Bidirectional, so output size will be 800)\n",
    "        self.lstm1 = nn.LSTM(input_size=hidden_size, hidden_size=400, batch_first=True, bidirectional=True)\n",
    "        # LSTM Layer 2 with 128 hidden units (Bidirectional, so output size will be 256)\n",
    "        self.lstm2 = nn.LSTM(input_size=800, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        # Dropout Layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # Dense Layer with ReLU activation to output a single score\n",
    "        # The input to the dense layer will have size 256 (output from second LSTM, bidirectional)\n",
    "        self.dense = nn.Linear(128 * 2, 1)  # Output layer for the score (128 * 2 for bidirectional)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first LSTM layer\n",
    "        x, _ = self.lstm1(x)  # Output size will be (batch_size, seq_len, 800) due to bidirectional LSTM\n",
    "        # Pass through second LSTM layer\n",
    "        x, _ = self.lstm2(x)  # Output size will be (batch_size, seq_len, 256) due to bidirectional LSTM\n",
    "        # Apply dropout to the output of the second LSTM\n",
    "        x = self.dropout(x)\n",
    "        # Take the output from the last sequence token (seq_length - 1)\n",
    "        x = x[:, -1, :]  # Select the last token (from sequence length)\n",
    "        # Apply dense layer and ReLU activation\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and move to device\n",
    "model_structure = StructureScoreModel(hidden_size=768).to(device)\n",
    "\n",
    "# Prepare structure scores for training\n",
    "structure_scores = torch.tensor(data_preprocessing['skor_struktur_normalized'].values, dtype=torch.float).to(device)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_scores, val_scores = train_test_split(\n",
    "    input_ids.cpu(), attention_mask.cpu(), structure_scores.cpu(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDataset and DataLoader for batching\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_scores)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_scores)\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression task\n",
    "optimizer = torch.optim.Adam(model_structure.parameters(), lr=1e-5)\n",
    "\n",
    "# Function to discretize continuous scores into categories\n",
    "def discretize_scores(scores, bins=10):\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    bin_edges = np.linspace(min_score, max_score, bins + 1)\n",
    "    discretized_scores = np.digitize(scores, bin_edges) - 1  # Subtract 1 to make categories 0-indexed\n",
    "    return discretized_scores\n",
    "\n",
    "# Function to scale the scores to a range of 1-10\n",
    "def scale_scores_to_1_10(scores):\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    scaled_scores = 1 + 9 * (scores - min_score) / (max_score - min_score)\n",
    "    return scaled_scores\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model_structure.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predicted_structure_scores_train = []\n",
    "    true_structure_scores_train = []\n",
    "    for batch in train_dataloader:\n",
    "        input_ids_batch, attention_mask_batch, structure_scores_batch = batch\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "        structure_scores_batch = structure_scores_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "        bert_embeddings = outputs.last_hidden_state\n",
    "        predictions = model_structure(bert_embeddings)\n",
    "\n",
    "        loss = criterion(predictions.squeeze(), structure_scores_batch.squeeze())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store predictions and true values for QWK calculation\n",
    "        predicted_structure_scores_train.extend(predictions.detach().cpu().numpy().flatten())  # Detach here\n",
    "        true_structure_scores_train.extend(structure_scores_batch.detach().cpu().numpy())  # Detach here\n",
    "\n",
    "    # Calculate QWK for training\n",
    "    predicted_structure_scores_train_scaled = scale_scores_to_1_10(predicted_structure_scores_train)\n",
    "    true_structure_scores_train_scaled = scale_scores_to_1_10(true_structure_scores_train)\n",
    "\n",
    "    qwk_train_score = cohen_kappa_score(discretize_scores(predicted_structure_scores_train_scaled),\n",
    "                                        discretize_scores(true_structure_scores_train_scaled), weights='quadratic')\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training QWK Score: {qwk_train_score:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model_structure.eval()\n",
    "    predicted_structure_scores_val = []\n",
    "    true_structure_scores_val = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids_batch, attention_mask_batch, structure_scores_batch = batch\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "            structure_scores_batch = structure_scores_batch.to(device)\n",
    "\n",
    "            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
    "            bert_embeddings = outputs.last_hidden_state\n",
    "\n",
    "            predictions = model_structure(bert_embeddings)\n",
    "\n",
    "            val_loss += criterion(predictions.squeeze(), structure_scores_batch.squeeze()).item()\n",
    "\n",
    "            # Store predictions and true values for QWK calculation\n",
    "            predicted_structure_scores_val.extend(predictions.detach().cpu().numpy().flatten())  # Detach here\n",
    "            true_structure_scores_val.extend(structure_scores_batch.detach().cpu().numpy())  # Detach here\n",
    "\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "    # Scale predictions to range 1-10 for validation\n",
    "    predicted_structure_scores_val_scaled = scale_scores_to_1_10(predicted_structure_scores_val)\n",
    "    true_structure_scores_val_scaled = scale_scores_to_1_10(true_structure_scores_val)\n",
    "\n",
    "    # Calculate QWK for validation\n",
    "    qwk_val_score = cohen_kappa_score(discretize_scores(predicted_structure_scores_val_scaled),\n",
    "                                    discretize_scores(true_structure_scores_val_scaled), weights='quadratic')\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation QWK Score: {qwk_val_score:.4f}\")\n",
    "\n",
    "\n",
    "# Save both raw and scaled predictions to CSV\n",
    "predictions_combined_df = pd.DataFrame({\n",
    "    'true_scores_raw': true_structure_scores_val,                # Actual scores (raw)\n",
    "    'predicted_scores_raw': predicted_structure_scores_val,  # Predicted scores (raw)\n",
    "    'true_scores_scaled': true_structure_scores_val_scaled,  # Actual scores (scaled)\n",
    "    'predicted_scores_scaled': predicted_structure_scores_val_scaled  # Predicted scores (scaled)\n",
    "})\n",
    "predictions_combined_df.to_csv('predicted_structure_scores_combined.csv', index=False)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
