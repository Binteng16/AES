{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library yang dibutuhkan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset yang sudah dipreproses sebelumnya\n",
    "data_preprocessing = pd.read_csv('pre_processing_data.csv')\n",
    "\"\"\"\n",
    "Baris ini membaca dataset hasil pre-processing dari file CSV bernama 'pre_processing_data.csv'\n",
    "menggunakan library pandas. Dataset ini akan digunakan untuk melatih model.\n",
    "\"\"\"\n",
    "\n",
    "# Inisialisasi tokenizer dan model BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\"\"\"\n",
    "Baris ini memuat tokenizer dan model BERT yang sudah dilatih sebelumnya ('bert-base-uncased').\n",
    "Tokenizer digunakan untuk memproses teks input menjadi token ID, sedangkan model BERT digunakan\n",
    "untuk menghasilkan representasi fitur dari teks input.\n",
    "\"\"\"\n",
    "\n",
    "# Memindahkan model BERT ke perangkat (CPU atau GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\"\"\"\n",
    "Baris ini memeriksa apakah GPU tersedia menggunakan torch.cuda.is_available().\n",
    "Jika tersedia, perangkat yang digunakan adalah GPU (CUDA); jika tidak, akan menggunakan CPU.\n",
    "Model BERT kemudian dipindahkan ke perangkat yang sesuai.\n",
    "\"\"\"\n",
    "\n",
    "# Ekstraksi teks esai dari dataset\n",
    "essay_texts = data_preprocessing['essay'].tolist()\n",
    "\"\"\"\n",
    "Baris ini mengekstrak kolom 'essay' dari dataset hasil pre-processing dan mengubahnya menjadi\n",
    "list Python menggunakan fungsi .tolist(). List ini berisi teks dari esai yang akan diproses.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenisasi teks esai dengan padding dan truncation\n",
    "inputs = tokenizer(essay_texts, return_tensors='pt', padding=True, max_length=512, truncation=True)\n",
    "\"\"\"\n",
    "Baris ini menggunakan tokenizer BERT untuk memproses teks esai:\n",
    "- `return_tensors='pt'`: Menghasilkan tensor PyTorch.\n",
    "- `padding=True`: Menambahkan padding agar semua teks memiliki panjang yang sama.\n",
    "- `max_length=512`: Memotong teks yang lebih panjang dari 512 token.\n",
    "- `truncation=True`: Mengaktifkan pemotongan teks yang terlalu panjang.\n",
    "\"\"\"\n",
    "\n",
    "# Ekstraksi token IDs dan attention mask\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\"\"\"\n",
    "Baris ini mengekstrak token IDs dan attention mask dari hasil tokenisasi.\n",
    "- `input_ids`: Tensor yang berisi ID token untuk setiap teks.\n",
    "- `attention_mask`: Tensor yang menunjukkan token mana yang relevan (1) dan mana yang berupa padding (0).\n",
    "\"\"\"\n",
    "\n",
    "# Memindahkan tokenized inputs ke perangkat yang sesuai\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\"\"\"\n",
    "Baris ini memindahkan tokenized inputs (input_ids dan attention_mask) ke perangkat yang digunakan (GPU atau CPU).\n",
    "\"\"\"\n",
    "\n",
    "# Mendefinisikan model untuk penilaian struktur esai\n",
    "class StructureScoreModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768):\n",
    "        \"\"\"\n",
    "        Konstruktor untuk model StructureScoreModel.\n",
    "        - `hidden_size=768`: Ukuran output dari layer BERT (default 768).\n",
    "        \"\"\"\n",
    "        super(StructureScoreModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=hidden_size, hidden_size=400, batch_first=True, bidirectional=True)\n",
    "        \"\"\"\n",
    "        Layer LSTM pertama:\n",
    "        - `input_size=hidden_size`: Inputnya adalah keluaran dari BERT (768).\n",
    "        - `hidden_size=400`: Ukuran hidden state (400).\n",
    "        - `batch_first=True`: Input/output memiliki dimensi (batch_size, seq_length, features).\n",
    "        - `bidirectional=True`: Menggunakan LSTM dua arah, sehingga outputnya menjadi (batch_size, seq_length, 800).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(input_size=800, hidden_size=128, batch_first=True, bidirectional=True)\n",
    "        \"\"\"\n",
    "        Layer LSTM kedua:\n",
    "        - `input_size=800`: Output dari layer LSTM pertama (400 * 2 untuk bidirectional).\n",
    "        - `hidden_size=128`: Ukuran hidden state (128).\n",
    "        - `batch_first=True`: Dimensi input/output tetap sama.\n",
    "        - `bidirectional=True`: Outputnya menjadi (batch_size, seq_length, 256).\n",
    "        \"\"\"\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \"\"\"\n",
    "        Dropout layer dengan probabilitas 0.5 untuk mencegah overfitting.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dense = nn.Linear(128 * 2, 1)\n",
    "        \"\"\"\n",
    "        Dense layer (fully connected):\n",
    "        - `input_size=256` (128 * 2 untuk bidirectional).\n",
    "        - `output_size=1`: Menghasilkan skor tunggal.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \"\"\"\n",
    "        Fungsi aktivasi ReLU untuk memastikan keluaran positif.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Fungsi forward untuk model StructureScoreModel.\n",
    "        - `x`: Input tensor dengan dimensi (batch_size, seq_length, features).\n",
    "        \"\"\"\n",
    "        x, _ = self.lstm1(x)\n",
    "        \"\"\"\n",
    "        Input melalui layer LSTM pertama. Output memiliki dimensi (batch_size, seq_length, 800).\n",
    "        \"\"\"\n",
    "        x, _ = self.lstm2(x)\n",
    "        \"\"\"\n",
    "        Output dari LSTM pertama masuk ke LSTM kedua. Dimensi output menjadi (batch_size, seq_length, 256).\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        \"\"\"\n",
    "        Output dari LSTM kedua dikenakan dropout.\n",
    "        \"\"\"\n",
    "        x = x[:, -1, :]\n",
    "        \"\"\"\n",
    "        Mengambil output token terakhir dari sequence (dimensi -1).\n",
    "        \"\"\"\n",
    "        x = self.dense(x)\n",
    "        \"\"\"\n",
    "        Output dari token terakhir dimasukkan ke dense layer untuk menghasilkan skor tunggal.\n",
    "        \"\"\"\n",
    "        x = self.relu(x)\n",
    "        \"\"\"\n",
    "        Output dari dense layer dikenakan fungsi aktivasi ReLU.\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "# Inisialisasi model dan pindahkan ke perangkat yang sesuai\n",
    "model_structure = StructureScoreModel(hidden_size=768).to(device)\n",
    "\"\"\"\n",
    "Baris ini membuat instance dari model StructureScoreModel dan memindahkannya ke perangkat yang digunakan.\n",
    "\"\"\"\n",
    "\n",
    "# Menyiapkan skor struktur untuk pelatihan\n",
    "structure_scores = torch.tensor(data_preprocessing['skor_struktur_normalized'].values, dtype=torch.float).to(device)\n",
    "\"\"\"\n",
    "Baris ini mengonversi skor struktur dari dataset hasil pre-processing menjadi tensor PyTorch dengan tipe data float\n",
    "dan memindahkannya ke perangkat yang digunakan (GPU atau CPU).\n",
    "\"\"\"\n",
    "\n",
    "# Membagi data menjadi set pelatihan dan validasi\n",
    "train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_scores, val_scores = train_test_split(\n",
    "    input_ids.cpu(), attention_mask.cpu(), structure_scores.cpu(), test_size=0.2, random_state=42)\n",
    "\"\"\"\n",
    "Baris ini membagi data menjadi set pelatihan dan validasi dengan rasio 80:20 menggunakan fungsi train_test_split.\n",
    "- `test_size=0.2`: 20% data digunakan untuk validasi.\n",
    "- `random_state=42`: Seed untuk memastikan hasil pembagian konsisten.\n",
    "\"\"\"\n",
    "\n",
    "# Membuat TensorDataset dan DataLoader untuk batching\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_scores)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_scores)\n",
    "batch_size = 2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\"\"\"\n",
    "Baris ini membuat dataset PyTorch (TensorDataset) dari data yang telah dibagi untuk pelatihan dan validasi.\n",
    "DataLoader digunakan untuk mengatur batching dan shuffle selama pelatihan.\n",
    "\"\"\"\n",
    "\n",
    "# Mendefinisikan fungsi loss dan optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\"\"\"\n",
    "Mean Squared Error (MSE) digunakan sebagai fungsi loss untuk tugas regresi ini.\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(model_structure.parameters(), lr=1e-5)\n",
    "\"\"\"\n",
    "Adam optimizer digunakan dengan learning rate sebesar 1e-5 untuk mengoptimalkan parameter model.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Fungsi untuk mendiskretkan skor kontinu ke dalam kategori definisi bin \"\"\"\n",
    "\"\"\" Parameter `bins` menentukan jumlah kategori. \"\"\"\n",
    "def discretize_scores(scores, bins=10):\n",
    "    \"\"\" Mendapatkan nilai minimum dan maksimum dari skor, lalu membaginya menjadi kategori. \"\"\"\n",
    "    min_score = np.min(scores)  # Mendapatkan nilai minimum dari skor\n",
    "    max_score = np.max(scores)  # Mendapatkan nilai maksimum dari skor\n",
    "    bin_edges = np.linspace(min_score, max_score, bins + 1)  # Membagi rentang skor ke dalam `bins` kategori\n",
    "    discretized_scores = np.digitize(scores, bin_edges) - 1  # Menghitung kategori untuk setiap skor (0-indexed)\n",
    "    return discretized_scores\n",
    "\n",
    "\"\"\" Parameter untuk pelatihan \"\"\"\n",
    "\"\"\" Jumlah epoch untuk melatih model. \"\"\"\n",
    "num_epochs = 50\n",
    "\n",
    "\"\"\" Training loop \"\"\"\n",
    "\"\"\" Proses pelatihan model selama sejumlah epoch. \"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    \"\"\" Mengatur model ke mode pelatihan dan memulai iterasi untuk setiap epoch. \"\"\"\n",
    "    model_structure.train()  # Mengatur model ke mode pelatihan\n",
    "    optimizer.zero_grad()  # Mengatur ulang gradien optimizer\n",
    "\n",
    "    predicted_structure_scores_train = []  # Menyimpan skor prediksi untuk data pelatihan\n",
    "    true_structure_scores_train = []  # Menyimpan skor sebenarnya untuk data pelatihan\n",
    "\n",
    "    for batch in train_dataloader:  # Melakukan iterasi pada setiap batch data pelatihan\n",
    "        \"\"\" Mengambil data input dan skor dari batch \"\"\"\n",
    "        input_ids_batch, attention_mask_batch, structure_scores_batch = batch\n",
    "\n",
    "        \"\"\" Memindahkan data input dan skor ke device (GPU atau CPU). \"\"\"\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_mask_batch = attention_mask_batch.to(device)\n",
    "        structure_scores_batch = structure_scores_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():  # Mematikan perhitungan gradien untuk model utama\n",
    "            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "        bert_embeddings = outputs.last_hidden_state  # Mengambil embedding dari output model\n",
    "        predictions = model_structure(bert_embeddings)  # Mendapatkan prediksi dari model struktur\n",
    "\n",
    "        \"\"\" Menghitung loss antara prediksi dan skor sebenarnya. \"\"\"\n",
    "        loss = criterion(predictions.squeeze(), structure_scores_batch.squeeze())\n",
    "\n",
    "        loss.backward()  # Melakukan backpropagation\n",
    "        optimizer.step()  # Memperbarui parameter model\n",
    "\n",
    "        \"\"\" Menyimpan prediksi dan skor sebenarnya untuk menghitung QWK. \"\"\"\n",
    "        predicted_structure_scores_train.extend(predictions.detach().cpu().numpy().flatten())\n",
    "        true_structure_scores_train.extend(structure_scores_batch.detach().cpu().numpy())\n",
    "\n",
    "    \"\"\" Menghitung QWK untuk data pelatihan \"\"\"\n",
    "    \"\"\" Skala prediksi dan skor sebenarnya ke dalam rentang 1-10 untuk menghitung QWK. \"\"\"\n",
    "\n",
    "    qwk_train_score = cohen_kappa_score(\n",
    "        discretize_scores(predicted_structure_scores_train),\n",
    "        discretize_scores(true_structure_scores_train),\n",
    "        weights='quadratic')\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training QWK Score: {qwk_train_score:.4f}\")\n",
    "\n",
    "    \"\"\" Validation loop \"\"\"\n",
    "    \"\"\" Proses validasi model untuk mengevaluasi performa. \"\"\"\n",
    "    model_structure.eval()  # Mengatur model ke mode evaluasi\n",
    "    predicted_structure_scores_val = []  # Menyimpan skor prediksi untuk data validasi\n",
    "    true_structure_scores_val = []  # Menyimpan skor sebenarnya untuk data validasi\n",
    "    val_loss = 0.0  # Inisialisasi nilai loss validasi\n",
    "\n",
    "    with torch.no_grad():  # Mematikan perhitungan gradien untuk validasi\n",
    "        for batch in val_dataloader:  # Iterasi pada setiap batch data validasi\n",
    "            \"\"\" Mengambil data input dan skor dari batch \"\"\"\n",
    "            input_ids_batch, attention_mask_batch, structure_scores_batch = batch\n",
    "\n",
    "            \"\"\" Memindahkan data input dan skor ke device (GPU atau CPU). \"\"\"\n",
    "            input_ids_batch = input_ids_batch.to(device)\n",
    "            attention_mask_batch = attention_mask_batch.to(device)\n",
    "            structure_scores_batch = structure_scores_batch.to(device)\n",
    "\n",
    "            outputs = model(input_ids_batch, attention_mask=attention_mask_batch)  # Mendapatkan output dari model\n",
    "            bert_embeddings = outputs.last_hidden_state  # Mengambil embedding dari output model\n",
    "\n",
    "            predictions = model_structure(bert_embeddings)  # Mendapatkan prediksi dari model struktur\n",
    "\n",
    "            \"\"\" Menghitung dan menjumlahkan loss untuk setiap batch. \"\"\"\n",
    "            val_loss += criterion(predictions.squeeze(), structure_scores_batch.squeeze()).item()\n",
    "\n",
    "            \"\"\" Menyimpan prediksi dan skor sebenarnya untuk menghitung QWK. \"\"\"\n",
    "            predicted_structure_scores_val.extend(predictions.detach().cpu().numpy().flatten())\n",
    "            true_structure_scores_val.extend(structure_scores_batch.detach().cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss / len(val_dataloader)  # Menghitung rata-rata loss validasi\n",
    "\n",
    "    qwk_val_score = cohen_kappa_score(\n",
    "        discretize_scores(predicted_structure_scores_val),\n",
    "        discretize_scores(true_structure_scores_val),\n",
    "        weights='quadratic')\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation QWK Score: {qwk_val_score:.4f}\")\n",
    "\n",
    "\"\"\" Menyimpan prediksi mentah dan tereskala ke dalam file CSV \"\"\"\n",
    "\"\"\" Menyimpan hasil prediksi ke file CSV untuk analisis lebih lanjut. \"\"\"\n",
    "predictions_combined_df = pd.DataFrame({\n",
    "    'true_scores_raw': true_structure_scores_val,  # Skor sebenarnya (mentah)\n",
    "    'predicted_scores_raw': predicted_structure_scores_val,  # Skor prediksi (mentah)\n",
    "    'true_scores_scaled': true_structure_scores_val,  # Skor sebenarnya\n",
    "    'predicted_scores_scaled': predicted_structure_scores_val  # Skor prediksi\n",
    "})\n",
    "\n",
    "\"\"\" Menyimpan dataframe ke file CSV \"\"\"\n",
    "\"\"\" Menyimpan dataframe yang telah berisi data prediksi dan skor sebenarnya ke dalam file CSV. \"\"\"\n",
    "predictions_combined_df.to_csv('predicted_structure_scores_combined.csv', index=False)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
